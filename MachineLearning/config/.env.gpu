# ============================================================================
# GPU Instance Environment Variables
# ============================================================================
# Configuration for GPU instances (Brev, AWS, GCP, Azure, etc.)
# Inherits from .env.common - only override what's different
# ============================================================================

# ----------------------------------------------------------------------------
# Environment Identifier
# ----------------------------------------------------------------------------
ENVIRONMENT=gpu

# ----------------------------------------------------------------------------
# CUDA/GPU Configuration (Multi-GPU support)
# ----------------------------------------------------------------------------
# Specify GPU devices to use (comma-separated for multiple)
CUDA_VISIBLE_DEVICES=0
# For multi-GPU: CUDA_VISIBLE_DEVICES=0,1,2,3

# Memory configuration
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Debug CUDA operations (disable in production)
# CUDA_LAUNCH_BLOCKING=1

# ----------------------------------------------------------------------------
# Training Configuration (Optimized for GPU)
# ----------------------------------------------------------------------------
DEFAULT_BATCH_SIZE=16
DEFAULT_EPOCHS=10
DEFAULT_LEARNING_RATE=5e-5
MAX_SEQ_LENGTH=512
GRADIENT_ACCUMULATION_STEPS=1
USE_AMP=true

# ----------------------------------------------------------------------------
# Dataset Configuration (Full scale)
# ----------------------------------------------------------------------------
MAX_FILES_PER_REPO=1000
MIN_FUNCTION_LENGTH=10
MAX_DATASET_SIZE=1000000

# ----------------------------------------------------------------------------
# Web Crawling Configuration
# ----------------------------------------------------------------------------
WEB_CRAWL_MAX_PAGES=500
CRAWL_TIMEOUT=60
CRAWL_DELAY=0.5

# ----------------------------------------------------------------------------
# Storage Sync Configuration
# ----------------------------------------------------------------------------
# Enable cloud sync for GPU instances
AUTO_SYNC_ON_STARTUP=true
AUTO_BACKUP_AFTER_TRAINING=true
AUTO_DOWNLOAD_DATASETS=true

# Sync intervals (seconds)
SYNC_INTERVAL=3600
BACKUP_INTERVAL=1800

# ----------------------------------------------------------------------------
# API Server Configuration (Production)
# ----------------------------------------------------------------------------
API_PORT=8000
API_HOST=0.0.0.0
ENABLE_AUTH=false
API_KEY=your_secure_api_key_here

# CORS for production
CORS_ORIGINS=*

# Request limits
MAX_REQUEST_SIZE=10485760
REQUEST_TIMEOUT=60

# ----------------------------------------------------------------------------
# Performance Configuration (Optimized for GPU)
# ----------------------------------------------------------------------------
NUM_WORKERS=4
PIN_MEMORY=true
CUDNN_BENCHMARK=true

# Optional: Enable torch.compile for PyTorch 2.0+
# TORCH_COMPILE=true

# ----------------------------------------------------------------------------
# Monitoring & Health Checks
# ----------------------------------------------------------------------------
ENABLE_METRICS=false
METRICS_PORT=9090
HEALTH_CHECK_INTERVAL=30
ENABLE_GPU_MONITORING=true

# ----------------------------------------------------------------------------
# Security Configuration
# ----------------------------------------------------------------------------
ENABLE_HTTPS=false
SSL_CERT_PATH=/etc/ssl/certs/cert.pem
SSL_KEY_PATH=/etc/ssl/private/key.pem

# Rate limiting (requests per minute per IP)
RATE_LIMIT=60
ENABLE_RATE_LIMIT=true

# ----------------------------------------------------------------------------
# Development Settings
# ----------------------------------------------------------------------------
DEBUG=false
AUTO_RELOAD=false
ENABLE_DOCS=true
ENABLE_PROFILING=false

# ----------------------------------------------------------------------------
# Logging Configuration
# ----------------------------------------------------------------------------
LOG_LEVEL=INFO
LOG_FILE=/var/log/ml_gpu_instance.log

# Separate logs for different components
TRAINING_LOG_FILE=/var/log/ml_training.log
INFERENCE_LOG_FILE=/var/log/ml_inference.log
API_LOG_FILE=/var/log/ml_api.log

# ----------------------------------------------------------------------------
# Model Configuration
# ----------------------------------------------------------------------------
# Use fast SSD for model cache
HF_HOME=/mnt/ssd/huggingface_cache

# Work offline after initial download
TRANSFORMERS_OFFLINE=false

# Disable symlinks warning
HF_HUB_DISABLE_SYMLINKS_WARNING=1

# ----------------------------------------------------------------------------
# Resource Limits
# ----------------------------------------------------------------------------
# Maximum GPU memory per process (MB)
MAX_GPU_MEMORY=16384

# Maximum CPU threads
OMP_NUM_THREADS=8

# Maximum memory for data loading (GB)
MAX_DATALOADER_MEMORY=8

# ----------------------------------------------------------------------------
# Distributed Training (Optional)
# ----------------------------------------------------------------------------
# Enable for multi-node training
DISTRIBUTED_TRAINING=false
MASTER_ADDR=localhost
MASTER_PORT=29500
WORLD_SIZE=1
RANK=0

# ----------------------------------------------------------------------------
# Experiment Tracking (Optional)
# ----------------------------------------------------------------------------
# WandB configuration
WANDB_API_KEY=
WANDB_PROJECT=ml_training
WANDB_ENTITY=

# MLflow configuration
MLFLOW_TRACKING_URI=
MLFLOW_EXPERIMENT_NAME=

# ----------------------------------------------------------------------------
# Notification Configuration (Optional)
# ----------------------------------------------------------------------------
# Slack webhook for notifications
SLACK_WEBHOOK_URL=

# Email notifications
SMTP_HOST=
SMTP_PORT=587
SMTP_USER=
SMTP_PASSWORD=
NOTIFICATION_EMAIL=

# ============================================================================
# End of GPU Instance Configuration
# ============================================================================