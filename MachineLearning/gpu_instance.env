# ============================================================================
# GPU Instance Environment Configuration
# ============================================================================
# This file contains environment variables specifically for GPU instances.
# Copy this to .env on your GPU instance and configure the values.
#
# Usage:
#   cp gpu_instance.env .env
#   nano .env  # Edit with your values
# ============================================================================

# ----------------------------------------------------------------------------
# GitHub Configuration (REQUIRED)
# ----------------------------------------------------------------------------
# GitHub personal access token for accessing private repositories and datasets
# Create at: https://github.com/settings/tokens
# Required scopes: repo (Full control of private repositories)
GITHUB_TOKEN=your_github_token_here

# ----------------------------------------------------------------------------
# GPU & CUDA Configuration
# ----------------------------------------------------------------------------
# Specify which GPU devices to use (comma-separated for multiple GPUs)
# Example: "0" for first GPU, "0,1" for first two GPUs
CUDA_VISIBLE_DEVICES=0

# PyTorch CUDA memory allocator configuration
# expandable_segments helps with memory fragmentation on GPU
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Enable CUDA launch blocking for better error messages (disable in production)
# CUDA_LAUNCH_BLOCKING=1

# ----------------------------------------------------------------------------
# Training Configuration
# ----------------------------------------------------------------------------
# Default batch size for training (increase for larger GPUs)
# Recommended: 4-8 for 16GB GPU, 16-32 for 24GB+ GPU
DEFAULT_BATCH_SIZE=8

# Number of training epochs
DEFAULT_EPOCHS=10

# Learning rate for training
DEFAULT_LEARNING_RATE=5e-5

# Maximum sequence length for tokenization
MAX_SEQ_LENGTH=512

# Gradient accumulation steps (for larger effective batch sizes)
GRADIENT_ACCUMULATION_STEPS=1

# Mixed precision training (automatic mixed precision)
USE_AMP=true

# ----------------------------------------------------------------------------
# API Server Configuration
# ----------------------------------------------------------------------------
# Port for the inference API server
API_PORT=8000

# Host for the API server (0.0.0.0 allows external connections)
API_HOST=0.0.0.0

# Enable authentication for API endpoints
ENABLE_AUTH=false

# API key for authentication (if ENABLE_AUTH=true)
# Generate a secure random key: python -c "import secrets; print(secrets.token_urlsafe(32))"
API_KEY=your_secure_api_key_here

# CORS allowed origins (comma-separated)
CORS_ORIGINS=*

# Maximum request size in bytes (for code/text inputs)
MAX_REQUEST_SIZE=1048576

# Request timeout in seconds
REQUEST_TIMEOUT=60

# ----------------------------------------------------------------------------
# Cloud Storage Configuration (OPTIONAL)
# ----------------------------------------------------------------------------
# Only needed if you want to backup models or download datasets from cloud

# Storage provider: backblaze, wasabi, s3, cloudflare_r2, digitalocean
STORAGE_PROVIDER=

# Backblaze B2
BACKBLAZE_KEY_ID=
BACKBLAZE_APPLICATION_KEY=
BACKBLAZE_BUCKET_NAME=

# Wasabi
WASABI_ACCESS_KEY=
WASABI_SECRET_KEY=
WASABI_BUCKET_NAME=
WASABI_REGION=us-east-1

# AWS S3
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_BUCKET_NAME=
AWS_REGION=us-east-1

# Cloudflare R2
CLOUDFLARE_ACCOUNT_ID=
CLOUDFLARE_ACCESS_KEY_ID=
CLOUDFLARE_SECRET_ACCESS_KEY=
CLOUDFLARE_BUCKET_NAME=

# DigitalOcean Spaces
DIGITALOCEAN_SPACES_KEY=
DIGITALOCEAN_SPACES_SECRET=
DIGITALOCEAN_SPACES_BUCKET=
DIGITALOCEAN_SPACES_REGION=nyc3

# Auto-backup models after training
AUTO_BACKUP_AFTER_TRAINING=true

# Auto-download datasets before training
AUTO_DOWNLOAD_DATASETS=false

# ----------------------------------------------------------------------------
# Logging Configuration
# ----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file path (leave empty to disable file logging)
LOG_FILE=logs/gpu_instance.log

# Enable colored terminal output
LOG_COLORS=true

# ----------------------------------------------------------------------------
# Model Configuration
# ----------------------------------------------------------------------------
# Model cache directory (for HuggingFace models)
HF_HOME=/tmp/huggingface_cache

# Disable HuggingFace telemetry
HF_HUB_DISABLE_TELEMETRY=1

# Use local files only (don't download from HuggingFace during inference)
TRANSFORMERS_OFFLINE=1

# ----------------------------------------------------------------------------
# Performance Optimization
# ----------------------------------------------------------------------------
# Number of data loading workers (0 = main process, 4-8 recommended for GPU)
NUM_WORKERS=4

# Pin memory for faster data transfer to GPU
PIN_MEMORY=true

# Enable torch.compile for faster inference (PyTorch 2.0+)
# TORCH_COMPILE=true

# Enable cuDNN benchmarking for faster training
CUDNN_BENCHMARK=true

# ----------------------------------------------------------------------------
# Monitoring & Health Checks
# ----------------------------------------------------------------------------
# Enable Prometheus metrics endpoint
ENABLE_METRICS=false

# Metrics port
METRICS_PORT=9090

# Health check interval in seconds
HEALTH_CHECK_INTERVAL=30

# ----------------------------------------------------------------------------
# Security
# ----------------------------------------------------------------------------
# Enable HTTPS (requires SSL certificates)
ENABLE_HTTPS=false

# SSL certificate paths
SSL_CERT_PATH=/path/to/cert.pem
SSL_KEY_PATH=/path/to/key.pem

# Rate limiting (requests per minute per IP)
RATE_LIMIT=60

# ----------------------------------------------------------------------------
# Development & Debugging
# ----------------------------------------------------------------------------
# Enable debug mode (more verbose logging, auto-reload)
DEBUG=false

# Enable FastAPI auto-reload on code changes
AUTO_RELOAD=false

# Enable API documentation endpoints (/docs, /redoc)
ENABLE_DOCS=true

# Enable profiling
ENABLE_PROFILING=false

# ============================================================================
# End of Configuration
# ============================================================================
